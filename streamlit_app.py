# homesç™»è¨˜é€£æºã‚¢ãƒ—ãƒª - Streamlit
# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼šv1.0ï¼ˆGoogle Sheetså¯¾å¿œ / HOMESï¼‹ç™»è¨˜PDFè§£æï¼‰

import streamlit as st
import pandas as pd
import fitz  # PyMuPDF
import requests
from bs4 import BeautifulSoup
import re
import datetime
import gspread
from google.oauth2.service_account import Credentials

# -----------------------------
# Streamlit UIï¼ˆåˆæœŸè¨­å®šï¼‰
# -----------------------------
st.set_page_config(page_title="HOMESç™»è¨˜é€£æºã‚¢ãƒ—ãƒª", layout="wide")
st.title("ğŸ  HOMESÃ—ç™»è¨˜ç°¿ è‡ªå‹•ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰è¨˜å…¥ã‚¢ãƒ—ãƒª")

# -----------------------------
# Google Sheetsèªè¨¼è¨­å®š
# -----------------------------
SCOPE = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# secrets.tomlã§å®šç¾©ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã‚’ä½¿ç”¨
credentials = Credentials.from_service_account_info(
    st.secrets["gcp_service_account"], scopes=SCOPE
)
gc = gspread.authorize(credentials)
SPREADSHEET_URL = st.secrets["other"]["spreadsheet_url"]
sh = gc.open_by_url(SPREADSHEET_URL)
ws = sh.sheet1

# -----------------------------
# æ©Ÿèƒ½ï¼šPDFã‹ã‚‰æƒ…å ±æŠ½å‡º
# -----------------------------
def extract_pdf_data(uploaded_file):
    text = fitz.open(stream=uploaded_file.read(), filetype="pdf").get_page_text(0)

    # ã‚ªãƒ¼ãƒŠãƒ¼å
    owner_match = re.search(r"æ‰€æœ‰è€…[\s\S]+?\n\s*(.+?)\s*\n", text)
    owner = owner_match.group(1).strip() if owner_match else "âŒï¼ˆè‡ªå‹•å–å¾—ä¸å¯ï¼‰"

    # å·å®¤
    room_match = re.search(r"å®¶å±‹ç•ªå·.*?ã®(\d{3})", text)
    room = room_match.group(1) if room_match else "âŒ"

    # åºŠé¢ç©ï¼ˆä¾‹ï¼šï¼’ï¼“ï¼šï¼‘ï¼˜ï¼‰
    floor_match = re.search(r"åºŠ\s*é¢\s*ç©[\s\S]+?(\d{1,3})[:ï¼š](\d{2})", text)
    floor_area = f"{floor_match.group(1)}.{floor_match.group(2)}" if floor_match else "âŒ"

    # æ•·åœ°æ¨©å‰²åˆ â†’ å°‚æœ‰é¢ç©è¨ˆç®—ç”¨
    shikichi = re.search(r"æ•·åœ°æ¨©.+?(\d{2,6})åˆ†ã®(\d{2,6})", text)
    if shikichi:
        numerator = int(shikichi.group(2))
        exclusive_area = round(numerator / 100, 2)
    else:
        exclusive_area = "âŒ"

    # å€Ÿå…¥é¡ãƒ»å€Ÿå…¥æ—¥
    loan_amt = re.search(r"å‚µæ¨©é¡\s*é‡‘([\dï¼Œ,]+)ä¸‡å††", text)
    loan_date = re.search(r"é‡‘éŠ­æ¶ˆè²»è²¸å€Ÿ.*?(ä»¤å’Œ|å¹³æˆ)(\d+)å¹´(\d+)æœˆ(\d+)æ—¥", text)

    if loan_amt:
        amount = int(loan_amt.group(1).replace("ï¼Œ", "").replace(",", ""))
    else:
        amount = None

    if loan_date:
        era = loan_date.group(1)
        year = int(loan_date.group(2)) + (2018 if era == "å¹³æˆ" else 0)
        if era == "ä»¤å’Œ": year += 2018  # ä»¤å’Œå…ƒå¹´=2019
        dt = datetime.date(year, int(loan_date.group(3)), int(loan_date.group(4)))
    else:
        dt = None

    return owner, room, floor_area, exclusive_area, amount, dt

# -----------------------------
# æ©Ÿèƒ½ï¼šHOMES URLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—
# -----------------------------
def extract_homes_data(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.content, "html.parser")
        text = soup.get_text()

        # ç‰©ä»¶å
        title_tag = soup.find("h1")
        name = title_tag.get_text().strip() if title_tag else "âŒ"

        # æ‰€åœ¨åœ°
        addr_match = re.search(r"æ‰€åœ¨åœ°\s*ï¼š\s*(æ±äº¬éƒ½.+?)\n", text)
        address = addr_match.group(1).strip() if addr_match else "âŒ"

        # é§…æƒ…å ±ï¼ˆ2ã¤ã¾ã§ï¼‰
        stations = re.findall(r"(.+?)é§…\s*å¾’æ­©(\d+)åˆ†", text)
        station1 = f"{stations[0][0]}é§… å¾’æ­©{stations[0][1]}åˆ†" if len(stations) > 0 else "âŒ"
        station2 = f"{stations[1][0]}é§… å¾’æ­©{stations[1][1]}åˆ†" if len(stations) > 1 else "âŒ"

        # éšå»ºã¦
        floor_match = re.search(r"éšå»ºã¦\s*ï¼š\s*(\d+)éšå»º", text)
        floors = floor_match.group(1) if floor_match else "âŒ"

        # ç·æˆ¸æ•°
        units_match = re.search(r"ç·æˆ¸æ•°\s*ï¼š\s*(\d+)æˆ¸", text)
        total_units = units_match.group(1) if units_match else "âŒ"

        return name, address, station1, station2, floors, total_units
    except:
        return "âŒ", "âŒ", "âŒ", "âŒ", "âŒ", "âŒ"

# -----------------------------
# UIï¼šãƒ•ã‚¡ã‚¤ãƒ«ãƒ»URLå…¥åŠ›æ¬„
# -----------------------------
with st.form("entry_form"):
    homes_url = st.text_input("HOMESã®URLã‚’å…¥åŠ›")
    uploaded_pdf = st.file_uploader("ç™»è¨˜ç°¿è¬„æœ¬ã®PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="pdf")
    submit = st.form_submit_button("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€")

if submit and homes_url and uploaded_pdf:
    with st.spinner("æƒ…å ±ã‚’æŠ½å‡ºä¸­..."):
        owner, room, floor_area, exclusive_area, loan_amount, loan_date = extract_pdf_data(uploaded_pdf)
        name, address, station1, station2, floors, total_units = extract_homes_data(homes_url)

        # çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ï¼ˆä¾‹ï¼šA2ï½ãªã©ã¯èª¿æ•´å¯ï¼‰
        ws.update("A2", [[owner]])
        ws.update("D2", [[name]])
        ws.update("J2", [[room]])
        ws.update("E7", [[floor_area]])
        ws.update("F7", [[exclusive_area]])
        ws.update("H7", [[total_units]])
        ws.update("J7", [[floors]])
        ws.update("D12", [[station1]])
        ws.update("H12", [[station2]])
        ws.update("D17", [[address]])
        st.success("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã—ãŸï¼")
